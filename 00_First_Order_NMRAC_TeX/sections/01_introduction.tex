\section{Introduction}
Model reference control is a technique that imposes a behavior on a system, by equating the the output of the model reference with the output of the controlled system. This way, the gains of a pre-defined controller structure can be found. 

However, this only works if the system's dynamics are known exactly, which is often far from reality. If the system's dynamics are not fully known, the controller gains can be found adaptively by a technique called \textit{model reference adaptive control} (MRAC). The adaptation law can be chosen in two ways, firstly a gradient descent-based update law as first proposed by Whitaker et al. \cite{whitaker1959adaptive} and applied in for example \textbf{wahby} \cite{bosshartComparisonTwoPID2021}, and secondly, a Lyapunov-based update law as proposed by Shackcloth et al. \cite{shackclothSynthesisModelReference1965}. The latter enforces that in every update step, we have a stabilizing controller, which is not guaranteed when using the former.

Since the structure of the control law is pre-defined, a rich enough model has found. To this end, neural networks (NN) are an ideal choice, since they are considered to be universal approximators \cite{hornikUniversalApproximationUnknown1990a}. Since at least the 90s, research has been conducted on NN controllers \cite{jiangBriefReviewNeural2017}, and it has been shown that they can learn a desired behavior, for example in \textbf{wahby} \cite{congPIDLikeNeuralNetwork2009,thanhNonlinearPIDControl2006,norrisNeuralNetworksControl2021}. However, standard NNCs are nonlinear, through the use of nonlinear activation functions, which makes their makes the formal verification of stability more challenging. Often these controllers rely on a posteriori stability verification, \hl{...}

which can become non-trivial or is not enough if we require online, stable learning techniques.

To ensure that a system engages in the desired behavior, we need to assess its stability, meaning we need to mathematically analyze and verify if a system converges to the desired output, given the input signal. If this is the case, we say the system is stable. While classical control theory primarily deals with linear time-invariant (LTI) systems, following the superposition principle, real-world systems often exhibit nonlinear behavior. This extends the scope of traditional control theory to handle these more complex, nonlinear systems, which leads us to not be able to use classical stability concepts, such as gain and phase margins. Hence, we need to use more generalized methods, such as Lyapunov theory, which aims to quantify dissipative energy in the system. However, finding an appropriate Lyapunov function can be challenging. When using MRACs with a Lyapunov-based update law, we can impose a Lyapunov function and ensure that the weights satisfy stability conditions on this function. In the MRAC literature however, this is only done for adaptive weights that appear in the form of a linear combination with the error dynamics of the system, meaning the learned parameters are not inside the nonlinear function. Hence, in this work we aim to extend on the MRAC principle and define a nonlinear update rule for MRAC controllers, where the estimated parameters are in a function that does not satisfy the superposition principle.

To this end, we first investigate the performance of stable linear MRACs compared to existing, not necessarily stable learning schematics, such as a PIDNN and a self-tuning PID controller, applied to an inverted pendulum, called Pendubot. With our insights on developing stable linear MRACs, we go ahead and tackle the problem of stable nonlinear MRACs. For reasons of simplicity, we begin by defining an update law for a first-order system and subsequently propose a MIMO update law. Both newly proposed update laws will are validated through simulations.